{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Face recognition - Questions Notebook.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NgR0j5310qqC"},"source":["# Face recognition\n","Task is to recognize a faces"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X_f3HHLmJIuT"},"source":["### Dataset\n","**Aligned Face Dataset from Pinterest**\n","\n","This dataset contains 10.770 images for 100 people. All images are taken from 'Pinterest' and      aligned using dlib library."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Fn8OlNXgMarq","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aV3hpS9ciSFr","outputId":"0bb071f9-2d02-4fc7-959b-e388a53ccd16","executionInfo":{"status":"ok","timestamp":1580214808299,"user_tz":-120,"elapsed":3232,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import tensorflow\n","tensorflow.__version__"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.1.0-rc1'"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CjRTlPkp1LC2"},"source":["#### Mount Google drive if you are using google colab\n","- We recommend using Google Colab as you can face memory issues and longer runtimes while running on local"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sBWMoTJ9cf3Z","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sO9mgMmp13sI"},"source":["#### Change current working directory to project folder"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TddMnf4D1-59","colab":{}},"source":["import os\n","#### Add your code here ####\n","os.chdir('/content/drive/My Drive/PGP-AIML-UT-Austin-Jun19/Computer Vision/Project/Face Recognition')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CBB_OncAQ8h_"},"source":["### Extract the zip file \n","- Extract Aligned Face Dataset from Pinterest.zip"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CI5uhBunLEZ9","colab":{}},"source":["import zipfile\n","with zipfile.ZipFile('/content/drive/My Drive/PGP-AIML-UT-Austin-Jun19/Computer Vision/Project/Face Recognition/Aligned Face Dataset from Pinterest.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/drive/My Drive/PGP-AIML-UT-Austin-Jun19/Computer Vision/Project/Face Recognition/')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oesXJD9ySB6w"},"source":["### Function to load images\n","- Define a function to load the images from the extracted folder and map each image with person id \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4Q7TS19vVbGb","outputId":"ecdc93d2-21fe-4d51-dd75-2ddb98bd6b25","executionInfo":{"status":"ok","timestamp":1580214812263,"user_tz":-120,"elapsed":1336,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["import numpy as np\n","import os\n","\n","class IdentityMetadata():\n","    def __init__(self, base, name, file):\n","        # print(base, name, file)\n","        # dataset base directory\n","        self.base = base\n","        # identity name\n","        self.name = name\n","        # image file name\n","        self.file = file\n","\n","    def __repr__(self):\n","        return self.image_path()\n","\n","    def image_path(self):\n","        return os.path.join(self.base, self.name, self.file) \n","    \n","def load_metadata(path):\n","    metadata = []\n","    for i in os.listdir(path):\n","        for f in os.listdir(os.path.join(path, i)):\n","            # Check file extension. Allow only jpg/jpeg' files.\n","            ext = os.path.splitext(f)[1]\n","            if ext == '.jpg' or ext == '.jpeg':\n","                metadata.append(IdentityMetadata(path, i, f))\n","    return np.array(metadata)\n","\n","# metadata = load_metadata('images')\n","metadata = load_metadata('PINS')\n","print(metadata)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[PINS/pins_anne hathaway/anne hathaway0.jpg\n"," PINS/pins_anne hathaway/anne hathaway1.jpg\n"," PINS/pins_anne hathaway/anne hathaway101.jpg ...\n"," PINS/pins_William Fichtner/William Fichtner99.jpg\n"," PINS/pins_William Fichtner/William Fichtner98.jpg\n"," PINS/pins_William Fichtner/William Fichtner97.jpg]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nG1Vzl3MPebA"},"source":["### Define function to load image\n","- Define a function to load image from the metadata"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ape5WxvVWKOe","colab":{}},"source":["import cv2\n","def load_image(path):\n","    img = cv2.imread(path, 1)\n","    # OpenCV loads images with color channels\n","    # in BGR order. So we need to reverse them\n","    return img[...,::-1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DYm-aYUDRANv"},"source":["#### Load a sample image \n","- Load one image using the function \"load_image\""]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ptDNq8noWK89","outputId":"a4d434bb-441e-47de-debe-b0e4d5d7e8b7","executionInfo":{"status":"ok","timestamp":1580214851271,"user_tz":-120,"elapsed":1108,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}},"colab":{"base_uri":"https://localhost:8080/","height":875}},"source":["#### Add your code here ####\n","load_image('PINS/pins_anne hathaway/anne hathaway0.jpg')"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[1, 0, 0],\n","        [1, 0, 0],\n","        [1, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[1, 0, 0],\n","        [1, 0, 0],\n","        [1, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       ...,\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yg0olr-8Xbqw"},"source":["### VGG Face model\n","- Here we are giving you the predefined model for VGG face"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hh0Pz6acuaDP","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n","\n","def vgg_face():\t\n","    model = Sequential()\n","    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n","    model.add(Convolution2D(64, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(128, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(128, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(256, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(ZeroPadding2D((1,1)))\n","    model.add(Convolution2D(512, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D((2,2), strides=(2,2)))\n","    \n","    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Convolution2D(2622, (1, 1)))\n","    model.add(Flatten())\n","    model.add(Activation('softmax'))\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j2JhG4NOe7vd"},"source":["#### Load the model \n","- Load the model defined above\n","- Then load the given weight file named \"vgg_face_weights.h5\""]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zAa3OASPvKac","colab":{}},"source":["#### Add your code here ####\n","from tensorflow.keras import backend\n","\n","#Initialize the model\n","model = vgg_face()\n","# model.fit(vgg_face, labels)\n","\n","#### Add your code here ####\n","import h5py\n","\n","# Close this file\n","# h5f.close()\n","\n","# Open the file as readonly.\n","h5f = h5py.File('vgg_face_weights.h5', 'r')\n","# model.load_weights('vgg_face_weights.h5')\n","\n","model.load_weights('vgg_face_weights.h5')\n","\n","# Check H5 Keys\n","list(h5f.keys())\n","\n","# Close this file\n","h5f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mStdpxzAf7y5"},"source":["### Get vgg_face_descriptor"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j9IQ9hcSwO9k","colab":{}},"source":["from tensorflow.keras.models import Model\n","vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LkBQRL_sd2U8"},"source":["### Generate embeddings for each image in the dataset\n","- Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B2yd69OydBAq","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"376f9d0f-277b-43f8-b358-31507bb99f5e","executionInfo":{"status":"ok","timestamp":1580214949989,"user_tz":-120,"elapsed":8960,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}}},"source":["# Get embedding vector for first image in the metadata using the pre-trained model\n","\n","img_path = metadata[0].image_path()\n","img = load_image(img_path)\n","\n","# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n","img = (img / 255.).astype(np.float32)\n","\n","img = cv2.resize(img, dsize = (224,224))\n","print(img.shape)\n","\n","# Obtain embedding vector for an image\n","# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \n","\n","embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n","print(embedding_vector.shape)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["(224, 224, 3)\n","(2622,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"plHvUTytcTGo"},"source":["### Generate embeddings for all images \n","- Write code to iterate through metadata and create embeddings for each image using `vgg_face_descriptor.predict()` and store in a list with name `embeddings`\n","\n","- If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 2622-zeroes as the final embedding from the model is of length 2622."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yY9ykxtueY4k","colab":{}},"source":["#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4hb3XSDsfTMG"},"source":["### Function to calculate distance between given 2 pairs of images.\n","\n","- Consider distance metric as \"Squared L2 distance\"\n","- Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0sNnRtt-U7aU","colab":{}},"source":["def distance(emb1, emb2):\n","    return np.sum(np.square(emb1 - emb2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JwVRkeoNUyUw"},"source":["#### Plot images and get distance between the pairs given below\n","- 2, 3 and 2, 180\n","- 30, 31 and 30, 100\n","- 70, 72 and 70, 115"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nDVLED10eboB","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","def show_pair(idx1, idx2):\n","    plt.figure(figsize=(8,3))\n","    plt.suptitle(f'Distance = {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n","    plt.subplot(121)\n","    plt.imshow(load_image(metadata[idx1].image_path()))\n","    plt.subplot(122)\n","    plt.imshow(load_image(metadata[idx2].image_path()));    \n","\n","show_pair(2, 3)\n","show_pair(2, 180)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-G2iDeWKYMae"},"source":["### Create train and test sets \n","- Create X_train, X_test and y_train, y_test\n","- Use train_idx to seperate out training features and labels\n","- Use test_idx to seperate out testing features and labels"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OThdBDPxYkd4","colab":{}},"source":["train_idx = np.arange(metadata.shape[0]) % 9 != 0\n","test_idx = np.arange(metadata.shape[0]) % 9 == 0\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DlYYwGQxXVwf"},"source":["### Encode the Labels \n","- Encode the targets\n","- Use LabelEncoder"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8GOQrjqeX2LZ","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"o9CylOWOa4xM"},"source":["### Standardize the feature values (3 marks)\n","- Scale the features using StandardScaler"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H7pUV0oYbLrR","colab":{}},"source":["# Standarize features\n","from sklearn.preprocessing import StandardScaler\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i2QukHGXbb6d"},"source":["### Reduce dimensions using PCA \n","- Reduce feature dimensions using Principal Component Analysis"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dVj1SSEebtG8","colab":{}},"source":["from sklearn.decomposition import PCA\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SzCsmZg8chW4"},"source":["### Build a Classifier \n","- Use SVM Classifier to predict the person in the given image\n","- Fit the classifier and print the score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MnBv9Ks0cwtA","colab":{}},"source":["from sklearn.svm import SVC\n","\n","#### Add your code here ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JGz1G8e3dUl5"},"source":["### Test results \n","- Take 10th image from test set and plot the image\n","- Report to which person(folder name in dataset) the image belongs to"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4zD_f8Sudeiw","colab":{}},"source":["import warnings\n","# Suppress LabelEncoder warning\n","warnings.filterwarnings('ignore')\n","\n","example_idx = 10\n","\n","example_image = load_image(metadata[test_idx][example_idx].image_path())\n","example_prediction = #### Add your code here ####\n","example_identity = encoder.inverse_transform(example_prediction)[0]\n","\n","plt.imshow(example_image)\n","plt.title(f'Identified as {example_identity}');"],"execution_count":0,"outputs":[]}]}