{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NLP_Project_Sarcasm_Detection_Questions-Solutions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pp68FAQf9aMN"},"source":["# Sarcasm Detection\n"," **Acknowledgement**\n","\n","Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019).\n","\n","**Required Files given in below link.**\n","\n","https://drive.google.com/drive/folders/1xUnF35naPGU63xwRDVGc-DkZ3M8V5mMk"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S3Wj_mIZ8S3K"},"source":["## Install `Tensorflow2.0` "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jW2Uk8otQvi8","colab":{}},"source":["!!pip uninstall tensorflow\n","!pip install tensorflow==2.0.0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v9kv9tyJ77eF"},"source":["## Get Required Files from Drive"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D0O_n6OIEVyL","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0mgRpOvFMjKR","colab":{}},"source":["#Set your project path \n","import os\n","project_path =  os.chdir('/content/drive/My Drive/PGP-AIML-UT-Austin-Jun19/NLP/Project')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WXYwajPeQbRq"},"source":["#**## Reading and Exploring Data**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vAk6BRUh8CqL"},"source":["## Read Data \"Sarcasm_Headlines_Dataset.json\". Explore the data and get  some insights about the data. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"StSLB-T8PuGr","colab":{}},"source":["import pandas as pd\n","df = pd.read_json ('https://raw.githubusercontent.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection/master/Sarcasm_Headlines_Dataset.json', lines=True)\n","df.tail()\n","\n","df['is_sarcastic'].value_counts()/len(df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIgU9aW2KiEM","colab_type":"code","colab":{}},"source":["df['is_sarcastic'].value_counts()/len(df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z6pXf7A78E2H"},"source":["## Drop `article_link` from dataset. \n","As we only need headline text data and is_sarcastic column for this project. We can drop artical link column here."]},{"cell_type":"code","metadata":{"id":"jYgcoIBIC-6g","colab_type":"code","colab":{}},"source":["df = df[['headline','is_sarcastic']]\n","df.tail()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VLSVsvrlP9qD","colab":{}},"source":["import seaborn as sns\n","\n","sns.countplot(df.is_sarcastic)\n","plt.xlabel('Label')\n","plt.title('Sarcasm vs Non-sarcasm')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D0h6IOxU8OdH"},"source":["## Get the Length of each line and find the maximum length.\n","As different lines are of different length. We need to pad the our sequences using the max length."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BRAsChZAQmr3","colab":{}},"source":["import re\n","\n","df['headline'] = df['headline'].apply(lambda x: x.lower())\n","df['headline'] = df['headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVI2LHdHGCVW","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJsXzkr6GP2f","colab_type":"code","colab":{}},"source":["word_idx = tokenizer.word_index\n","idx_word = tokenizer.index_word"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfwmcVDkGUo2","colab_type":"code","colab":{}},"source":["sequences = tokenizer.texts_to_sequences(df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GguL3jkZtAU9","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","X = df[\"headline\"]\n","y = df[\"is_sarcastic\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, \n","                                                          random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, \n","                                                          random_state=52)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAgWHvQltO6i","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n","model.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acX1YrjDtSD-","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","\n","print(f'Accuracy on train set: {accuracy_score(y_train, model.predict(X_train))}')\n","print(f'Accuracy on validation set: {accuracy_score(y_val, model.predict(X_val))}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvlFJTKAoNGe","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer(lower=False)\n","tokenizer.fit_on_texts(X_train)\n","\n","word_index = tokenizer.word_index\n","print(f\"Found {len(word_index)} unique tokens.\")\n","\n","sequences_train = tokenizer.texts_to_sequences(X_train)\n","sequences_test = tokenizer.texts_to_sequences(X_test)\n","sequences_val = tokenizer.texts_to_sequences(X_val)\n","\n","max_len = max([len(x) for x in sequences_train+sequences_val+sequences_test])\n","print(f\"The longest headline has {max_len} words.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VPPd0YuPXi2M"},"source":["#**## Modelling**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"35abKfRx8as3"},"source":["## Import required modules required for modelling."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DVel73hYEV4r","colab":{}},"source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n","from tensorflow.keras.models import Model, Sequential"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9ziybaD1RdD9"},"source":["## Set Different Parameters for the model. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jPw9gAN_EV6m","colab":{}},"source":["max_features = 10000\n","maxlen = 135 ## Add your max length here ##\n","embedding_size = 200"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9abSe-bM8fn9"},"source":["## Apply Keras Tokenizer of headline column of your data. \n","Hint - First create a tokenizer instance using Tokenizer(num_words=max_features) \n","And then fit this tokenizer instance on your data column df['headline'] using .fit_on_texts()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T9Ad26HfTFMS","colab":{}},"source":["tokenizer = Tokenizer(maxlen=max_features)\n","tokenizer.fit_on_texts(X_train)\n","\n","word_index = tokenizer.word_index\n","print(f\"Found {len(word_index)} unique tokens.\")\n","\n","sequences_train = tokenizer.texts_to_sequences(X_train)\n","sequences_test = tokenizer.texts_to_sequences(X_test)\n","sequences_val = tokenizer.texts_to_sequences(X_val)\n","\n","max_len = max([len(x) for x in sequences_train+sequences_val+sequences_test])\n","print(f\"The longest headline contains {max_len} words.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Ffi63KsST3P"},"source":["## Define X and y for your model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wnjxBdqmSS4s","colab":{}},"source":["X = tokenizer.texts_to_sequences(df['headline'])\n","X = pad_sequences(X, maxlen = maxlen)\n","y = np.asarray(df['is_sarcastic'])\n","\n","print(\"Number of Samples:\", len(X))\n","print(X[0])\n","print(\"Number of Labels: \", len(y))\n","print(y[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WJLyKg-98rH_"},"source":["## Get the Vocabulary size ( 4 marks)\n","Hint : You can use tokenizer.word_index."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q-2w0gHEUUIo","colab":{}},"source":["word_index = tokenizer.word_index\n","print(f\"Found {len(word_index)} unique tokens.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5hjeMi40XcB1"},"source":["#**## Word Embedding**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bUF1TuQa8ux0"},"source":["## Get Glove Word Embeddings"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vq5AIfRtMeZh","colab":{}},"source":["glove_file = project_path + \"./glove.6B.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DJLX_n2WMecA","colab":{}},"source":["#Extract Glove embedding zip file\n","from zipfile import ZipFile\n","with ZipFile(glove_file, 'r') as z:\n","  z.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9IuXlu8-U3HG"},"source":["# Get the Word Embeddings using Embedding file as given below."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"elZ-T5aFGZmZ","colab":{}},"source":["EMBEDDING_FILE = './glove.6B.200d.txt'\n","\n","embeddings = {}\n","for o in open(EMBEDDING_FILE):\n","    word = o.split(\" \")[0]\n","    # print(word)\n","    embd = o.split(\" \")[1:]\n","    embd = np.asarray(embd, dtype='float32')\n","    # print(embd)\n","    embeddings[word] = embd\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bTPxveDmVCrA"},"source":["# Create a weight matrix for words in training docs"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xQgOhiywU9nU","colab":{}},"source":["embedding_matrix = np.zeros((num_words, 200))\n","\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","len(embeddings.values())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u7IbWuEX82Ra"},"source":["## Create and Compile your Model \n","Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, then dense and dropout layers as required. \n","In the end add a final dense layer with sigmoid activation for binary classification.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d7jhsSgYXG4l","colab":{}},"source":["### Embedding layer for hint \n","## model.add(Embedding(num_words, embedding_size, weights = [embedding_matrix]))\n","### Bidirectional LSTM layer for hint \n","## model.add(Bidirectional(LSTM(128, return_sequences = True)))\n","vocab_size = 25343\n","import tensorflow as tf\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_size, input_length=maxlen),\n","    Dropout(0.2),\n","    Bidirectional(LSTM(64, return_sequences = True)),\n","    Bidirectional(LSTM(32)),\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXaEJ7aMt7gq","colab_type":"code","colab":{}},"source":["from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","\n","SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IJFMxZwMWoTw"},"source":["## Fit your model with a batch size of 100 and validation_split = 0.2. and state the validation accuracy "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZpVkajCcWnRK","colab":{}},"source":["batch_size = 100\n","epochs = 5\n","\n","## Add your code here ##\n","BiLSTM = model.fit(train_padded, train_labels, epochs = num_epochs, \n","                    validation_data = (test_padded, test_labels), verbose =2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7nlk_VhuzJr","colab_type":"code","colab":{}},"source":["score,acc = BiLSTM.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVywZ2T-u0Np","colab_type":"code","colab":{}},"source":["pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n","for x in range(len(X_test)):\n","    \n","    result = BiLSTM.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n","   \n","    if np.argmax(result) == np.argmax(Y_test[x]):\n","        if np.argmax(Y_test[x]) == 0:\n","            neg_correct += 1\n","        else:\n","            pos_correct += 1\n","       \n","    if np.argmax(Y_test[x]) == 0:\n","        neg_cnt += 1\n","    else:\n","        pos_cnt += 1\n","\n","\n","\n","print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n","print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTPkQD9Lu9fQ","colab_type":"code","colab":{}},"source":["scores = BiLSTM.evaluate(X_train, y_train, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgIiwGZzvfeb","colab_type":"code","colab":{}},"source":["scores = BiLSTM.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFFPPEcywI8r","colab_type":"code","colab":{}},"source":["plt.plot(BiLSTM.history['loss'], label='train loss')\n","plt.plot(BiLSTM.history['val_loss'], label='val loss')\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"Cross-entropy loss\")\n","plt.legend();"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGcZ1OBXwM6J","colab_type":"code","colab":{}},"source":["plt.plot(BiLSTM.history['accuracy'], label='train accuracy')\n","plt.plot(BiLSTM.history['val_accuracy'], label='val accuracy')\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"accuracy\")\n","plt.legend();"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoKRs_k0wP8J","colab_type":"code","colab":{}},"source":["BiLSTM.evaluate(X_test, y_test)"],"execution_count":0,"outputs":[]}]}