{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGyHWbOM70HN"
   },
   "source": [
    "## Image Classification\n",
    "\n",
    "\n",
    "Image classification is one of the important use cases in our daily life. Automotive, e-commerce, retail, manufacturing industries, security, surveillance, healthcare, farming etc., can have a wide application of image classification.\n",
    "\n",
    "**Objective:** In this notebook, we will build a neural network to classifiy the image based on the object present in the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LSdjqV35-j_"
   },
   "source": [
    "\n",
    "## Advanced techniques for training neural networks\n",
    "\n",
    "Weight Initialization\n",
    "\n",
    "Nonlinearity (different Activation functions)\n",
    "\n",
    "Optimizers(different optimizers)\n",
    "\n",
    "Batch Normalization\n",
    "\n",
    "Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfL8u4jx82pG"
   },
   "source": [
    "### About Dataset\n",
    "\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "#### Labels\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "0 T-shirt/top\n",
    "\n",
    "1 Trouser\n",
    "\n",
    "2 Pullover\n",
    "\n",
    "3 Dress\n",
    "\n",
    "4 Coat\n",
    "\n",
    "5 Sandal\n",
    "\n",
    "6 Shirt\n",
    "\n",
    "7 Sneaker\n",
    "\n",
    "8 Bag\n",
    "\n",
    "9 Ankle boot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XGznu8t6HBc"
   },
   "source": [
    "### Load dataset\n",
    "\n",
    "Fashion-MNIST dataset\n",
    "\n",
    "source: https://www.kaggle.com/zalando-research/fashionmnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3m64izx05_VU",
    "outputId": "8fafea48-5541-4368-a8e9-d7ebdabd14e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "eAkQmLqP6I2W",
    "outputId": "ce7c329a-93a5-4be0-f2bc-d61678c8f25c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 3us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "Mr5bH4OY6Ndk",
    "outputId": "23fe3726-011b-45d0-b982-8bff2dc15b9f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  9\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_train[0])    # show first number in the dataset\n",
    "plt.show()\n",
    "print('Label: ', y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "bcTZqYFj6PcB",
    "outputId": "c79ae5dd-15c7-4b44-b232-bb1c4653f271"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EBS/wfAkh/XIL+CnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  9\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_test[0])    # show first number in the dataset\n",
    "plt.show()\n",
    "print('Label: ', y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZxmqqAa9Cz7"
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TCY3Jnm6RHr"
   },
   "outputs": [],
   "source": [
    "# reshaping X data: (n, 28, 28) => (n, 784)\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oG06URdH6Vvv"
   },
   "outputs": [],
   "source": [
    "# converting y data into categorical (one-hot encoding)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "71HK9T0P6Y2G",
    "outputId": "f9b4fcb4-7f78-4a67-e228-b298ee8a1b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784) (60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yb0Kx_ME6plD"
   },
   "source": [
    "### Basic NN model\n",
    "\n",
    "Naive MLP model without any alterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weHT53gF6aFY"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "izZlXqJu6sdo"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dCi-i296tuq"
   },
   "outputs": [],
   "source": [
    "  model.add(Dense(50, input_shape = (784, )))\n",
    "  model.add(Activation('sigmoid'))\n",
    "  model.add(Dense(50))\n",
    "  model.add(Activation('sigmoid'))\n",
    "  model.add(Dense(50))\n",
    "  model.add(Activation('sigmoid'))\n",
    "  model.add(Dense(50))\n",
    "  model.add(Activation('sigmoid'))\n",
    "  model.add(Dense(10))\n",
    "  model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJ5FUE916x2b"
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "Zl34l0jR6zH-",
    "outputId": "7766ffcb-47e1-42ab-d84f-a860a9716ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 2.3176 - acc: 0.1287\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2931 - acc: 0.2033\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2897 - acc: 0.2139\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2862 - acc: 0.2768\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2822 - acc: 0.2978\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2775 - acc: 0.3063\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2718 - acc: 0.3042\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2647 - acc: 0.3345\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2554 - acc: 0.3743\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2435 - acc: 0.3676\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2275 - acc: 0.3714\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2054 - acc: 0.3344\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1739 - acc: 0.3398\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1286 - acc: 0.3221\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.0654 - acc: 0.2882\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.9857 - acc: 0.3067\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.9000 - acc: 0.2986\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.8237 - acc: 0.3081\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.7650 - acc: 0.3400\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.7216 - acc: 0.3592\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.6877 - acc: 0.3637\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.6582 - acc: 0.3782\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.6296 - acc: 0.3929\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.6010 - acc: 0.4136\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.5717 - acc: 0.4386\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.5412 - acc: 0.4561\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.5084 - acc: 0.4933\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.4746 - acc: 0.5017\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.4406 - acc: 0.5127\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.4075 - acc: 0.5104\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.3757 - acc: 0.5333\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.3432 - acc: 0.5352\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 1.3116 - acc: 0.5409\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 1.2797 - acc: 0.5528\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 1.2503 - acc: 0.5405\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 1.2226 - acc: 0.5584\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 1.1963 - acc: 0.5655\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 1.1720 - acc: 0.5688\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 1.1522 - acc: 0.5689\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.1311 - acc: 0.5761\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.1138 - acc: 0.5783\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.0972 - acc: 0.5818\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.0802 - acc: 0.5827\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.0657 - acc: 0.5844\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.0508 - acc: 0.5929\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.0348 - acc: 0.5943\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.0212 - acc: 0.5962\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 1.0065 - acc: 0.6053\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9895 - acc: 0.6129\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9740 - acc: 0.6235\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9635 - acc: 0.6328\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9501 - acc: 0.6355\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9366 - acc: 0.6345\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9214 - acc: 0.6478\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.9087 - acc: 0.6539\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.8985 - acc: 0.6579\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.8873 - acc: 0.6643\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.8800 - acc: 0.6677\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.8655 - acc: 0.6722\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.8574 - acc: 0.6707\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.8487 - acc: 0.6771\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.8385 - acc: 0.6856\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.8231 - acc: 0.6979\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.8145 - acc: 0.6983\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.8104 - acc: 0.6997\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7982 - acc: 0.7102\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7953 - acc: 0.7169\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7851 - acc: 0.7214\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7789 - acc: 0.7201\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7741 - acc: 0.7210\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7701 - acc: 0.7236\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7643 - acc: 0.7203\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7550 - acc: 0.7350\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7477 - acc: 0.7382\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7489 - acc: 0.7312\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7456 - acc: 0.7367\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7361 - acc: 0.7404\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7336 - acc: 0.7452\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7286 - acc: 0.7473\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7333 - acc: 0.7407\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7259 - acc: 0.7491\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7163 - acc: 0.7555\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7077 - acc: 0.7588\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.7086 - acc: 0.7602\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.7020 - acc: 0.7618\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.6913 - acc: 0.7690\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.6992 - acc: 0.7624\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.6976 - acc: 0.7647\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.6889 - acc: 0.7665\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.6903 - acc: 0.7677\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.6829 - acc: 0.7705\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.6797 - acc: 0.7727\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.6703 - acc: 0.7775\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.6681 - acc: 0.7761\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.6665 - acc: 0.7804\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.6604 - acc: 0.7829\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.6661 - acc: 0.7783\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.6603 - acc: 0.7765\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.6516 - acc: 0.7851\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.6652 - acc: 0.7801\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rhl0xqgR62ei",
    "outputId": "5dce908f-7f02-4c1d-ea04-ff6c455c4ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 57us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVi98CNA7bxe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6PSheNKy66Iu",
    "outputId": "7c5807bb-f821-4274-962a-b904a043526f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7749\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V7kN5-f7IHv"
   },
   "source": [
    "### 1. Weight Initialization\n",
    "\n",
    "Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree\n",
    "\n",
    "Ref: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7il0ZpZZ67GP"
   },
   "outputs": [],
   "source": [
    "# from now on, create a function to generate (return) models\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, ), kernel_initializer='he_normal'))     # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3417
    },
    "colab_type": "code",
    "id": "9iOQeOzR7Q2O",
    "outputId": "4d27aad3-ac31-4227-d421-511d7be5d004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 2.4905 - acc: 0.1000\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.3740 - acc: 0.1000\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.3281 - acc: 0.1000\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.3080 - acc: 0.1010\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2989 - acc: 0.1205\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2944 - acc: 0.1573\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2920 - acc: 0.2066\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2905 - acc: 0.2394\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2894 - acc: 0.2739\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2884 - acc: 0.2997\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2876 - acc: 0.3160\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2868 - acc: 0.3382\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2860 - acc: 0.3664\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2852 - acc: 0.3702\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2844 - acc: 0.3744\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2837 - acc: 0.3877\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2830 - acc: 0.4065\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2822 - acc: 0.3986\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2815 - acc: 0.4271\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2807 - acc: 0.4101\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2800 - acc: 0.4365\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2793 - acc: 0.4431\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2786 - acc: 0.4424\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2778 - acc: 0.4444\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2771 - acc: 0.4500\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2764 - acc: 0.4397\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2756 - acc: 0.4636\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2748 - acc: 0.4672\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2741 - acc: 0.4718\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2733 - acc: 0.4722\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2726 - acc: 0.4924\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2718 - acc: 0.4870\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2711 - acc: 0.5017\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2703 - acc: 0.4923\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2696 - acc: 0.4887\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2688 - acc: 0.5072\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2680 - acc: 0.4940\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2673 - acc: 0.5096\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2665 - acc: 0.5030\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2656 - acc: 0.5123\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2648 - acc: 0.5168\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2640 - acc: 0.5167\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2631 - acc: 0.5143\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2622 - acc: 0.5260\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2613 - acc: 0.5235\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2604 - acc: 0.5275\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2595 - acc: 0.5213\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2586 - acc: 0.5289\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2576 - acc: 0.5306\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2566 - acc: 0.5294\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2556 - acc: 0.5285\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2546 - acc: 0.5408\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2536 - acc: 0.5292\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2525 - acc: 0.5410\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2514 - acc: 0.5461\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2503 - acc: 0.5472\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2492 - acc: 0.5519\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2480 - acc: 0.5515\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2468 - acc: 0.5489\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2456 - acc: 0.5475\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2444 - acc: 0.5496\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2432 - acc: 0.5457\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2419 - acc: 0.5611\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2406 - acc: 0.5612\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2393 - acc: 0.5578\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2379 - acc: 0.5582\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2365 - acc: 0.5528\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2350 - acc: 0.5577\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2335 - acc: 0.5541\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2320 - acc: 0.5538\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2304 - acc: 0.5609\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2287 - acc: 0.5571\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2270 - acc: 0.5653\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2252 - acc: 0.5569\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2234 - acc: 0.5551\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2215 - acc: 0.5551\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2197 - acc: 0.5533\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2178 - acc: 0.5642\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2158 - acc: 0.5557\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2138 - acc: 0.5614\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2117 - acc: 0.5599\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2096 - acc: 0.5532\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2074 - acc: 0.5595\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2051 - acc: 0.5546\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2027 - acc: 0.5547\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.2003 - acc: 0.5599\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1978 - acc: 0.5648\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1952 - acc: 0.5519\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1925 - acc: 0.5522\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1898 - acc: 0.5628\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1871 - acc: 0.5603\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1842 - acc: 0.5500\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1813 - acc: 0.5531\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1782 - acc: 0.5533\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1752 - acc: 0.5508\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1720 - acc: 0.5480\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1687 - acc: 0.5472\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1653 - acc: 0.5508\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.1618 - acc: 0.5480\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 2.1581 - acc: 0.5466\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ThB85Acn7SFu",
    "outputId": "e4924ece-0e32-41f7-bce3-affbc46d507c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 54us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iq5a7na67UgU",
    "outputId": "d4203af4-ae95-43df-ce8c-b3b3a1252320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.5497\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5M6EHv17YUV"
   },
   "source": [
    "### 2. Nonlinearity (Activation function)\n",
    "\n",
    "Sigmoid functions suffer from gradient vanishing problem, making training slower\n",
    "\n",
    "There are many choices apart from sigmoid and tanh; try many of them!\n",
    "\n",
    "'relu' (rectified linear unit) is one of the most popular ones\n",
    "\n",
    "Ref: https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuuRRvxL7WU1"
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "LHqen6477eRZ",
    "outputId": "1b7d82d9-8795-41f7-f54a-3bef7cf1eeea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 6.6232 - acc: 0.4382\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.6876 - acc: 0.7587\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.5735 - acc: 0.7972\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.5295 - acc: 0.8101\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5013 - acc: 0.8189\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4809 - acc: 0.8258\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4658 - acc: 0.8295\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.4532 - acc: 0.8355\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4428 - acc: 0.8383\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.4331 - acc: 0.8409\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LLrEe_Kn7fha",
    "outputId": "47d6c862-b477-44cc-c1df-fdfaf2aaaaa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 53us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BBUqrqSd7heP",
    "outputId": "6c771e1d-60dd-4914-c152-a83b57918b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.815\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lp_-o2ug70fF"
   },
   "source": [
    "### 3. Batch Normalization\n",
    "\n",
    "Batch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective\n",
    "\n",
    "Normalize each mini-batch before nonlinearity\n",
    "\n",
    "Ref: https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "buhn2kOY7yg8"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YrSd0LT78Yi"
   },
   "source": [
    "Batch normalization layer is usually inserted after dense/convolution and before nonlinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10InY0e_77M7"
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "jOCywkTg79_W",
    "outputId": "848852f0-a3bb-4c05-e5aa-c47113cbcc68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 16s 264us/step - loss: 1.3378 - acc: 0.5872\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.8461 - acc: 0.7412\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.7146 - acc: 0.7725\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.6482 - acc: 0.7892\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.6021 - acc: 0.8005\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 0.5737 - acc: 0.8088\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.5467 - acc: 0.8146\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.5264 - acc: 0.8215\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 0.5104 - acc: 0.8253\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.4964 - acc: 0.8303\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.4879 - acc: 0.8332\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.4750 - acc: 0.8360\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.4662 - acc: 0.8388\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.4576 - acc: 0.8414\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.4523 - acc: 0.8427\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.4412 - acc: 0.8466\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 0.4361 - acc: 0.8470\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.4302 - acc: 0.8495\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.4240 - acc: 0.8521\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.4186 - acc: 0.8559\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 20, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gGodrhsc8AW7",
    "outputId": "7c35554c-5cca-42c7-fa66-7cb0c3365db1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 92us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zDnDxm8r8Bt0",
    "outputId": "cf73192d-0568-4119-c8d4-86177d39840f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8588\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElSuYRBH8yim"
   },
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKXR7REP8C8s"
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, ), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "8FJsCqh680bq",
    "outputId": "dbd7c421-b484-42b8-9d23-afc60fdb380a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 20s 334us/step - loss: 0.8453 - acc: 0.7140\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.5950 - acc: 0.7980\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 0.5447 - acc: 0.8143\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 20s 333us/step - loss: 0.5138 - acc: 0.8240\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.4906 - acc: 0.8302\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 19s 317us/step - loss: 0.4807 - acc: 0.8339\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 0.4639 - acc: 0.8394\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 0.4592 - acc: 0.8435\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.4466 - acc: 0.8446\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 19s 316us/step - loss: 0.4404 - acc: 0.8486\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TY58JhC085IM",
    "outputId": "fa423f1b-6ccb-40e6-c2c5-b8562a506872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 106us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SU5NUjzs85KT",
    "outputId": "254e8bc0-235b-48bd-f080-294fec0a832d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8653\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image_Classification_Intro_to_NN_3.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
