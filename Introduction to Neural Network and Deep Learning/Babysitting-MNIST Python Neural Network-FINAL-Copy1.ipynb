{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"},"colab":{"name":"Babysitting-MNIST Python Neural Network-FINAL-Copy1.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"XkDd2G21ovJ4","colab_type":"text"},"source":["### Baby-sitting the Learning Process"]},{"cell_type":"markdown","metadata":{"id":"MVVx4PDPovJ6","colab_type":"text"},"source":["#### Defining the Fully Connected Layer"]},{"cell_type":"code","metadata":{"id":"VBB96E7KovJ7","colab_type":"code","colab":{}},"source":["import numpy as np \n","\n","class Linear():\n","    def __init__(self, in_size, out_size):\n","        self.W = np.random.randn(in_size, out_size) * 0.01\n","        self.b = np.zeros((1, out_size))\n","        self.params = [self.W, self.b]\n","        self.gradW = None\n","        self.gradB = None\n","        self.gradInput = None\n","\n","    def forward(self, X):\n","        self.X = X\n","        output = np.dot(self.X, self.W) + self.b\n","        return output\n","\n","    def backward(self, nextgrad):\n","        self.gradW = np.dot(self.X.T, nextgrad)\n","        self.gradB = np.sum(nextgrad, axis=0)\n","        self.gradInput = np.dot(nextgrad, self.W.T)\n","        return self.gradInput, [self.gradW, self.gradB]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P4inlKo8ovJ9","colab_type":"text"},"source":["#### Defining the Rectified Linear Activation Layer\n"]},{"cell_type":"code","metadata":{"id":"hF8ySv1JovJ9","colab_type":"code","colab":{}},"source":["class ReLU():\n","    def __init__(self):\n","        self.params = []\n","        self.gradInput = None\n","\n","    def forward(self, X):\n","        self.output = np.maximum(X, 0)\n","        return self.output\n","\n","    def backward(self, nextgrad):\n","        self.gradInput = nextgrad.copy()\n","        self.gradInput[self.output <=0] = 0\n","        return self.gradInput, []"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8WszN3lWovJ_","colab_type":"text"},"source":["#### Defining the softmax function"]},{"cell_type":"code","metadata":{"id":"THpGEE2dovKA","colab_type":"code","colab":{}},"source":["def softmax(x):\n","    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n","    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5fJTeJNRovKB","colab_type":"text"},"source":["#### Defining the Cross Entropy Loss"]},{"cell_type":"code","metadata":{"id":"GaUvHPytovKC","colab_type":"code","colab":{}},"source":["class CrossEntropy:\n","    def forward(self, X, y):\n","        self.m = y.shape[0]\n","        self.p = softmax(X)\n","        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n","        loss = np.sum(cross_entropy) / self.m\n","        return loss\n","    \n","    def backward(self, X, y):\n","        y_idx = y.argmax()        \n","        grad = softmax(X)\n","        grad[range(self.m), y] -= 1\n","        grad /= self.m\n","        return grad"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEDd_85oovKE","colab_type":"text"},"source":["### Loading the dataset : Lets us load the training and the test data and check the size of the tensors. Lets us also display the first few images from the training set."]},{"cell_type":"code","metadata":{"id":"UmOQNIOTovKE","colab_type":"code","outputId":"e85f1dbc-6835-44b7-eb8d-686f88769029","executionInfo":{"status":"ok","timestamp":1575059322234,"user_tz":-120,"elapsed":2407,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}},"colab":{"base_uri":"https://localhost:8080/","height":83}},"source":["from keras.datasets import mnist\n","from keras.utils import np_utils\n","\n","\n","(train_features, train_targets), (test_features, test_targets) = mnist.load_data()\n","\n","train_features = train_features.reshape(60000, 784)\n","print train_features.shape\n","test_features = test_features.reshape(10000, 784)\n","print test_features.shape\n","\n","\n","# # normalize inputs from 0-255 to 0-1\n","train_features = train_features / 255.0\n","test_features = test_features / 255.0\n","\n","print train_targets.shape\n","print test_targets.shape\n","\n","X_train = train_features\n","y_train = train_targets\n","\n","X_val = test_features\n","y_val = test_targets"],"execution_count":53,"outputs":[{"output_type":"stream","text":["(60000, 784)\n","(10000, 784)\n","(60000,)\n","(10000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K8C19hsoovKH","colab_type":"code","outputId":"7eb2d445-f73b-4480-f6cf-a5031ce373ab","executionInfo":{"status":"ok","timestamp":1575059322234,"user_tz":-120,"elapsed":2405,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}},"colab":{"base_uri":"https://localhost:8080/","height":115}},"source":["# visualizing the first 10 images in the dataset and their labels\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 1))\n","for i in range(10):\n","    plt.subplot(1, 10, i+1)\n","    plt.imshow(X_train[i].reshape(28, 28), cmap=\"gray\")\n","    plt.axis('off')\n","plt.show()\n","print('label for each of the above image: %s' % (y_train[0:10]))"],"execution_count":54,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlMAAABSCAYAAABwglFkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 720x72 with 10 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["label for each of the above image: [5 0 4 1 9 2 1 3 1 4]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZpUHSOH3ovKI","colab_type":"text"},"source":["### Here, I define the container NN class that enables the forward prop and backward propagation of the entire network. Note, how this class enables us to add layers of different types and also correctly pass gradients using the chain rule."]},{"cell_type":"code","metadata":{"id":"eS_6lmAhovKJ","colab_type":"code","colab":{}},"source":["class NN():\n","    def __init__(self, lossfunc=CrossEntropy(), mode='train'):\n","        self.params = []\n","        self.layers = []\n","        self.loss_func = lossfunc\n","        self.grads = []\n","        self.mode = mode\n","        \n","    def add_layer(self, layer):\n","        self.layers.append(layer)\n","        self.params.append(layer.params)\n","\n","    def forward(self, X):\n","        for layer in self.layers:\n","            X = layer.forward(X)\n","        return X\n","    \n","    def backward(self, nextgrad):\n","        self.clear_grad_param()\n","        for layer in reversed(self.layers):\n","            nextgrad, grad = layer.backward(nextgrad)\n","            self.grads.append(grad)\n","        return self.grads\n","    \n","    def train_step(self, X, y):\n","        out = self.forward(X)\n","        loss = self.loss_func.forward(out,y)  + ((Lambda / (2 * y.shape[0])) * np.sum([np.sum(w**2) for w in self.params[0][0]]))\n","        nextgrad = self.loss_func.backward(out,y) + ((Lambda/y.shape[0]) * np.sum([np.sum(w) for w in self.params[0][0]]))\n","        grads = self.backward(nextgrad)\n","        return loss, grads\n","    \n","    def predict(self, X):\n","        X = self.forward(X)\n","        p = softmax(X)\n","        return np.argmax(p, axis=1)\n","    \n","    def predict_scores(self, X):\n","        X = self.forward(X)\n","        p = softmax(X)\n","        return p\n","    \n","    def clear_grad_param(self):\n","        self.grads = []"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6LERe7PdovKK","colab_type":"text"},"source":["#### Defining the update function (SGD)"]},{"cell_type":"code","metadata":{"id":"oABEigo7ovKL","colab_type":"code","colab":{}},"source":["def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\n","    for v, p, g, in zip(velocity, params, reversed(grads)):\n","        for i in range(len(g)):\n","            v[i] = (mu * v[i]) - (learning_rate * g[i])\n","            p[i] += v[i]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ilpYs26novKN","colab_type":"text"},"source":["#### Defining a function which gives us the minibatches (both the datapoint and the corresponding label)"]},{"cell_type":"code","metadata":{"id":"PTiz0gpAovKN","colab_type":"code","colab":{}},"source":["# get minibatches\n","def minibatch(X, y, minibatch_size):\n","    n = X.shape[0]\n","    minibatches = []\n","    permutation = np.random.permutation(X.shape[0])\n","    X = X[permutation]\n","    y = y[permutation]\n","    \n","    for i in range(0, n , minibatch_size):\n","        X_batch = X[i:i + minibatch_size, :]\n","        y_batch = y[i:i + minibatch_size, ]\n","\n","        minibatches.append((X_batch, y_batch))\n","    return minibatches"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f3qv8whgovKP","colab_type":"text"},"source":["#### The traning loop"]},{"cell_type":"code","metadata":{"id":"EJiLTVpPovKP","colab_type":"code","colab":{}},"source":["def sgd(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu=0.9, X_val=None, y_val=None, Lambda=0, verb=True):\n","    val_loss_epoch = []\n","    minibatches = minibatch(X_train, y_train, minibatch_size)\n","    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n","    \n","    for i in range(epoch):\n","        loss_batch = []\n","        val_loss_batch = []\n","        velocity = []\n","        for param_layer in net.params:\n","            p = [np.zeros_like(param) for param in list(param_layer)]\n","            velocity.append(p)\n","            \n","        # iterate over mini batches\n","        for X_mini, y_mini in minibatches:\n","            loss, grads = net.train_step(X_mini, y_mini)\n","            loss_batch.append(loss)\n","            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n","\n","        for X_mini_val, y_mini_val in minibatches_val:\n","            val_loss, _ = net.train_step(X_mini, y_mini)\n","            val_loss_batch.append(val_loss)\n","        \n","        # accuracy of model at end of epoch after all mini batch updates\n","        m_train = X_train.shape[0]\n","        m_val = X_val.shape[0]\n","        y_train_pred = []\n","        y_val_pred = []\n","        y_train1 = []\n","        y_vall = []\n","        for ii in range(0, m_train, minibatch_size):\n","            X_tr = X_train[ii:ii + minibatch_size, : ]\n","            y_tr = y_train[ii:ii + minibatch_size,]\n","            y_train1 = np.append(y_train1, y_tr)\n","            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n","\n","        for ii in range(0, m_val, minibatch_size):\n","            X_va = X_val[ii:ii + minibatch_size, : ]\n","            y_va = y_val[ii:ii + minibatch_size,]\n","            y_vall = np.append(y_vall, y_va)\n","            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n","            \n","        train_acc = check_accuracy(y_train1, y_train_pred)\n","        val_acc = check_accuracy(y_vall, y_val_pred)\n","        \n","        ## weights\n","        w = np.array(net.params[0][0])\n","        \n","        ## adding regularization to cost\n","        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\n","        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n","        \n","        val_loss_epoch.append(mean_val_loss)\n","        if verb:\n","            if i%50==0:\n","                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\n","    return net, val_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEKFnVx9ovKR","colab_type":"text"},"source":["#### Checking the accuracy of the model "]},{"cell_type":"code","metadata":{"id":"g7bz3frGovKR","colab_type":"code","colab":{}},"source":["def check_accuracy(y_true, y_pred):\n","    count = 0\n","    for i,j in zip(y_true, y_pred):\n","        if int(i)==j:\n","            count +=1\n","    return float(count)/float(len(y_true))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MUecjD5LovKT","colab_type":"text"},"source":["#### Invoking all that we have created until now"]},{"cell_type":"code","metadata":{"id":"gU9IWDkOovKU","colab_type":"code","colab":{}},"source":["from random import shuffle\n","\n","\n","## input size\n","input_dim = X_train.shape[1]\n","\n","def train_and_test_loop(iterations, lr, Lambda, verb=True):\n","    ## hyperparameters\n","    iterations = iterations\n","    learning_rate = lr\n","    hidden_nodes = 10\n","    output_nodes = 10\n","\n","    ## define neural net\n","    nn = NN()\n","    nn.add_layer(Linear(input_dim, hidden_nodes))\n","\n","    nn, val_acc = sgd(nn, X_train , y_train, minibatch_size=1000, epoch=iterations, learning_rate=learning_rate,\\\n","                      X_val=X_val, y_val=y_val, Lambda=Lambda, verb=verb)\n","    return val_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phKanLVcovKV","colab_type":"text"},"source":["### Double Check that the loss is reasonable : Disable the regularization"]},{"cell_type":"code","metadata":{"id":"c-dYuXPDovKW","colab_type":"code","colab":{}},"source":["# lr = 0.00001\n","# Lambda = 0\n","# train_and_test_loop(1, lr, Lambda)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VSpcMT04ovKX","colab_type":"text"},"source":["Is the loss range correct?? What about accuracy, does it make sense for an untrained network?"]},{"cell_type":"markdown","metadata":{"id":"B8pYI05vovKY","colab_type":"text"},"source":["### Now, lets crank up the Lambda(Regularization)and check what it does to our loss function."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"r6pN9ziWovKY","colab_type":"code","colab":{}},"source":["# lr = 0.00001\n","# Lambda = 1e3\n","# train_and_test_loop(1, lr, Lambda)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WfSURdmHovKa","colab_type":"text"},"source":["loss went up. Good! (Another sanity check)"]},{"cell_type":"markdown","metadata":{"id":"YAn3pdSWovKb","colab_type":"text"},"source":["### Now, lets overfit to a small subset of our dataset, in this case 20 images."]},{"cell_type":"code","metadata":{"id":"sWTdw_VcovKb","colab_type":"code","colab":{}},"source":["X_train_subset = X_train[0:20]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hq42ZEsMovKe","colab_type":"code","colab":{}},"source":["y_train_subset = y_train[0:20]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkjolKNOovKh","colab_type":"code","colab":{}},"source":["X_train = X_train_subset\n","y_train = y_train_subset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xn3JuSSKovKj","colab_type":"code","outputId":"3638835e-264f-40f1-debc-0432905c24bb","executionInfo":{"status":"ok","timestamp":1575059322240,"user_tz":-120,"elapsed":2385,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["X_train.shape"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20, 784)"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"_av1W1tyovKk","colab_type":"code","outputId":"e5fea9b1-4bbb-4a98-c9e0-24d765d6e8f7","executionInfo":{"status":"ok","timestamp":1575059322240,"user_tz":-120,"elapsed":2383,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["y_train.shape"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20,)"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"markdown","metadata":{"id":"g1wanXBwovKm","colab_type":"text"},"source":["In the code below:\n","- Take the first 20 examples from MNIST\n","- turn off regularization(reg=0.0)\n","- use simple vanilla 'sgd'"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"mj07YlpBovKn","colab_type":"code","colab":{}},"source":["# lr = 0.0001\n","# Lambda = 0\n","# train_and_test_loop(10000, lr, Lambda)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLpGcPwRovKo","colab_type":"text"},"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"fKxNRNPSovKp","colab_type":"text"},"source":["### Loading the original dataset again"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"CzrIoVA4ovKp","colab_type":"code","outputId":"89c1397f-751c-4ecf-cbb6-a201f77b13a0","executionInfo":{"status":"ok","timestamp":1575059322730,"user_tz":-120,"elapsed":2869,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}},"colab":{"base_uri":"https://localhost:8080/","height":83}},"source":["import pandas as pd\n","from keras.datasets import mnist\n","from keras.utils import np_utils\n","\n","\n","(train_features, train_targets), (test_features, test_targets) = mnist.load_data()\n","\n","train_features = train_features.reshape(60000, 784)\n","print train_features.shape\n","test_features = test_features.reshape(10000, 784)\n","print test_features.shape\n","\n","\n","# # normalize inputs from 0-255 to 0-1\n","train_features = train_features / 255.0\n","test_features = test_features / 255.0\n","\n","print train_targets.shape\n","print test_targets.shape\n","\n","X_train = train_features\n","y_train = train_targets\n","\n","X_val = test_features\n","y_val = test_targets"],"execution_count":69,"outputs":[{"output_type":"stream","text":["(60000, 784)\n","(10000, 784)\n","(60000,)\n","(10000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PmsK2ybrovKr","colab_type":"text"},"source":["### Start with small regularization and find learning rate that makes the loss go down.\n","\n","- start with Lambda(small regularization) = 1e-7\n","- start with a small learning rate = 1e-7"]},{"cell_type":"code","metadata":{"id":"WnGE_qxfovKs","colab_type":"code","colab":{}},"source":["# lr = 1e-7\n","# Lambda = 1e-7\n","# train_and_test_loop(500, lr, Lambda)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADBLVHGhovKw","colab_type":"text"},"source":["Loss barely changing. Learning rate is probably too low."]},{"cell_type":"markdown","metadata":{"id":"G0VpcZaLovKw","colab_type":"text"},"source":["### Okay now lets try a (larger) learning rate 1e6. What could possibly go wrong?\n","\n","- Learning rate lr = 1e6\n","- Regularization lambda = 1e-7\n"]},{"cell_type":"code","metadata":{"id":"Bg5TLSzzovKx","colab_type":"code","colab":{}},"source":["# lr = 1e6\n","# Lambda = 1e-7\n","# train_and_test_loop(500, lr, Lambda)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIBN6BAEovKz","colab_type":"text"},"source":["Loss exploding. Learning rate is too high. \n","Cost is very high. Always means high learning rate"]},{"cell_type":"markdown","metadata":{"id":"Ddm2aVBoovK0","colab_type":"text"},"source":["### Lets try to train now with a value of learning rate between 1e-7 and 1e6\n","\n","- learning rate = 1e4\n","- regularization remains the small, lambda = 1e-7"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NLPPKv8_ovK1","colab_type":"code","colab":{}},"source":["# lr = 1e4\n","# Lambda = 1e-7\n","# train_and_test_loop(500, lr, Lambda)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UWHO3U4HovK2","colab_type":"text"},"source":["Still too high learning rate. Loss is not decreasing. The rough range of learning rate we should be cross validating is somewhere between [1e4 to 1e-7]"]},{"cell_type":"markdown","metadata":{"id":"lp0-7eq6ovK3","colab_type":"text"},"source":["### Hyperparameter Optimization\n","\n","### Cross validation Strategy\n","\n","\n","- Do coarse -> fine cross-validation in stages\n","\n","- First stage: only a few epochs to get rough idea of what params work\n","- Second stage: longer running time, finer search\n","- … (repeat as necessary)\n","\n","### Tip for detecting explosions in the solver: \n","- If the cost is ever > 3 * original cost, break out early\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"v_H-PADUovK3","colab_type":"text"},"source":["### For example: Run coarse search for 100 iterations\n"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"9dTBaXTUovK4","colab_type":"code","colab":{}},"source":["# import math\n","# for k in range(1,100):\n","#     lr = math.pow(10, np.random.uniform(-7.0, 4.0))\n","#     Lambda = math.pow(10, np.random.uniform(-5,5))\n","#     best_acc = train_and_test_loop(100, lr, Lambda, False)\n","#     print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxmVkfXDovK5","colab_type":"text"},"source":["### Now run finer search"]},{"cell_type":"code","metadata":{"id":"hqeiCBjyovK6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":366},"outputId":"0bb8cfef-2198-4dc1-d4bb-ece731366454","executionInfo":{"status":"ok","timestamp":1575060789704,"user_tz":-120,"elapsed":1469833,"user":{"displayName":"Daniel Schoeman","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ0GG-kReNY5wx1Yj_KzqpFUwYFhI7f5WbTKcWMA=s64","userId":"17687301418143097392"}}},"source":["import math\n","for k in range(1,10):\n","    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\n","    Lambda = math.pow(10, np.random.uniform(-5,2))\n","    best_acc = train_and_test_loop(100, lr, Lambda, False)\n","    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"],"execution_count":74,"outputs":[{"output_type":"stream","text":["Try 1/100: Best_val_acc: 0.9082, lr: 0.00162322285494, Lambda: 0.565211556146\n","\n","Try 2/100: Best_val_acc: 0.9177, lr: 0.00520748540154, Lambda: 3.05566843017e-05\n","\n","Try 3/100: Best_val_acc: 0.9093, lr: 0.00187245310409, Lambda: 0.000306033780085\n","\n","Try 4/100: Best_val_acc: 0.9065, lr: 0.00148196872012, Lambda: 3.07636783878e-05\n","\n","Try 5/100: Best_val_acc: 0.9043, lr: 0.00117401597512, Lambda: 1.097682628e-05\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in square\n","/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in subtract\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Try 6/100: Best_val_acc: 0.098, lr: 0.00161439611737, Lambda: 3.40594576735\n","\n","Try 7/100: Best_val_acc: 0.098, lr: 0.00106264850247, Lambda: 20.6484697727\n","\n","Try 8/100: Best_val_acc: 0.9091, lr: 0.00183642210165, Lambda: 0.198382458737\n","\n","Try 9/100: Best_val_acc: 0.098, lr: 0.00498456291836, Lambda: 0.992582560927\n","\n"],"name":"stdout"}]}]}